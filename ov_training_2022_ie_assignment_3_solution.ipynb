{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9604082b",
   "metadata": {},
   "source": [
    "# 歡迎來到 **使用 benchmark app 觀察深度學習模型效能與初探\u000b",
    "OpenVINO Runtime API(推論引擎)效能增強工具** 課程的編程作業!\n",
    "\n",
    "你將通過完成這份作業, 更加熟悉[OpenVINO Benchmark Python* Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html)\n",
    "\n",
    "來溫習一下benchmark app吧, 它能夠協助你\n",
    "- 觀察深度學習模型效能\n",
    "- 它是 OpenVINO 的通用基準測試工具, 可以運行任何模型\n",
    "- 不需要考慮輸入和輸出binary large object, 簡單容易使用\n",
    "- 測試的時候不需要輸入數據, 只需要input model和inference device即可, 非常智能\n",
    "- 能夠自動為平台選用最佳的參數, 方便, 支持許多optional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba1a34",
   "metadata": {},
   "source": [
    "執行如下cell參考指令用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7978424b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "usage: benchmark_app [-h [HELP]] [-i PATHS_TO_INPUT [PATHS_TO_INPUT ...]] -m\n",
      "                     PATH_TO_MODEL [-d TARGET_DEVICE] [-l PATH_TO_EXTENSION]\n",
      "                     [-c PATH_TO_CLDNN_CONFIG]\n",
      "                     [-hint {throughput,latency,none}] [-api {sync,async}]\n",
      "                     [-niter NUMBER_ITERATIONS] [-nireq NUMBER_INFER_REQUESTS]\n",
      "                     [-b BATCH_SIZE] [-stream_output [STREAM_OUTPUT]]\n",
      "                     [-t TIME] [-progress [PROGRESS]] [-shape SHAPE]\n",
      "                     [-data_shape DATA_SHAPE] [-layout LAYOUT]\n",
      "                     [-nstreams NUMBER_STREAMS]\n",
      "                     [--latency_percentile {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100}]\n",
      "                     [-enforcebf16 [{True,False}]] [-nthreads NUMBER_THREADS]\n",
      "                     [-pin {YES,NO,NUMA,HYBRID_AWARE}]\n",
      "                     [-exec_graph_path EXEC_GRAPH_PATH] [-pc [PERF_COUNTS]]\n",
      "                     [-pcseq [PCSEQ]] [-inference_only [INFERENCE_ONLY]]\n",
      "                     [-report_type {no_counters,average_counters,detailed_counters}]\n",
      "                     [-report_folder REPORT_FOLDER] [-dump_config DUMP_CONFIG]\n",
      "                     [-load_config LOAD_CONFIG] [-qb {8,16}]\n",
      "                     [-ip {u8,U8,f16,FP16,f32,FP32}]\n",
      "                     [-op {u8,U8,f16,FP16,f32,FP32}]\n",
      "                     [-iop INPUT_OUTPUT_PRECISION] [-cdir CACHE_DIR]\n",
      "                     [-lfile [LOAD_FROM_FILE]] [-iscale INPUT_SCALE]\n",
      "                     [-imean INPUT_MEAN]\n",
      "\n",
      "Options:\n",
      "  -h [HELP], --help [HELP]\n",
      "                        Show this help message and exit.\n",
      "  -i PATHS_TO_INPUT [PATHS_TO_INPUT ...], --paths_to_input PATHS_TO_INPUT [PATHS_TO_INPUT ...]\n",
      "                        Optional. Path to a folder with images and/or binaries\n",
      "                        or to specific image or binary file.It is also allowed\n",
      "                        to map files to network inputs:\n",
      "                        input_1:file_1/dir1,file_2/dir2,input_4:file_4/dir4\n",
      "                        input_2:file_3/dir3\n",
      "  -m PATH_TO_MODEL, --path_to_model PATH_TO_MODEL\n",
      "                        Required. Path to an .xml/.onnx file with a trained\n",
      "                        model or to a .blob file with a trained compiled\n",
      "                        model.\n",
      "  -d TARGET_DEVICE, --target_device TARGET_DEVICE\n",
      "                        Optional. Specify a target device to infer on (the\n",
      "                        list of available devices is shown below). Default\n",
      "                        value is CPU. Use '-d HETERO:<comma separated devices\n",
      "                        list>' format to specify HETERO plugin. Use '-d\n",
      "                        MULTI:<comma separated devices list>' format to\n",
      "                        specify MULTI plugin. The application looks for a\n",
      "                        suitable plugin for the specified device.\n",
      "  -l PATH_TO_EXTENSION, --path_to_extension PATH_TO_EXTENSION\n",
      "                        Optional. Required for CPU custom layers. Absolute\n",
      "                        path to a shared library with the kernels\n",
      "                        implementations.\n",
      "  -c PATH_TO_CLDNN_CONFIG, --path_to_cldnn_config PATH_TO_CLDNN_CONFIG\n",
      "                        Optional. Required for GPU custom kernels. Absolute\n",
      "                        path to an .xml file with the kernels description.\n",
      "  -hint {throughput,latency,none}, --perf_hint {throughput,latency,none}\n",
      "                        Optional. Performance hint (latency or throughput or\n",
      "                        none). Performance hint allows the OpenVINO device to\n",
      "                        select the right network-specific settings.\n",
      "                        'throughput': device performance mode will be set to\n",
      "                        THROUGHPUT. 'latency': device performance mode will be\n",
      "                        set to LATENCY. 'none': no device performance mode\n",
      "                        will be set. Using explicit 'nstreams' or other\n",
      "                        device-specific options, please set hint to 'none'\n",
      "  -api {sync,async}, --api_type {sync,async}\n",
      "                        Optional. Enable using sync/async API. Default value\n",
      "                        is async.\n",
      "  -niter NUMBER_ITERATIONS, --number_iterations NUMBER_ITERATIONS\n",
      "                        Optional. Number of iterations. If not specified, the\n",
      "                        number of iterations is calculated depending on a\n",
      "                        device.\n",
      "  -nireq NUMBER_INFER_REQUESTS, --number_infer_requests NUMBER_INFER_REQUESTS\n",
      "                        Optional. Number of infer requests. Default value is\n",
      "                        determined automatically for device.\n",
      "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
      "                        Optional. Batch size value. If not specified, the\n",
      "                        batch size value is determined from Intermediate\n",
      "                        Representation\n",
      "  -stream_output [STREAM_OUTPUT]\n",
      "                        Optional. Print progress as a plain text. When\n",
      "                        specified, an interactive progress bar is replaced\n",
      "                        with a multi-line output.\n",
      "  -t TIME, --time TIME  Optional. Time in seconds to execute topology.\n",
      "  -progress [PROGRESS]  Optional. Show progress bar (can affect performance\n",
      "                        measurement). Default values is 'False'.\n",
      "  -shape SHAPE          Optional. Set shape for input. For example,\n",
      "                        \"input1[1,3,224,224],input2[1,4]\" or \"[1,3,224,224]\"\n",
      "                        in case of one input size.This parameter affect model\n",
      "                        Parameter shape, can be dynamic. For dynamic dimesions\n",
      "                        use symbol `?`, `-1` or range `low.. up`.\n",
      "  -data_shape DATA_SHAPE\n",
      "                        Optional. Optional if network shapes are all static\n",
      "                        (original ones or set by -shape).Required if at least\n",
      "                        one input shape is dynamic and input images are not\n",
      "                        provided.Set shape for input tensors. For example,\n",
      "                        \"input1[1,3,224,224][1,3,448,448],input2[1,4][1,8]\" or\n",
      "                        \"[1,3,224,224][1,3,448,448] in case of one input size.\n",
      "  -layout LAYOUT        Optional. Prompts how network layouts should be\n",
      "                        treated by application. For example,\n",
      "                        \"input1[NCHW],input2[NC]\" or \"[NCHW]\" in case of one\n",
      "                        input size.\n",
      "  -nstreams NUMBER_STREAMS, --number_streams NUMBER_STREAMS\n",
      "                        Optional. Number of streams to use for inference on\n",
      "                        the CPU/GPU/MYRIAD (for HETERO and MULTI device cases\n",
      "                        use format <device1>:<nstreams1>,<device2>:<nstreams2>\n",
      "                        or just <nstreams>). Default value is determined\n",
      "                        automatically for a device. Please note that although\n",
      "                        the automatic selection usually provides a reasonable\n",
      "                        performance, it still may be non - optimal for some\n",
      "                        cases, especially for very small networks. Also, using\n",
      "                        nstreams>1 is inherently throughput-oriented option,\n",
      "                        while for the best-latency estimations the number of\n",
      "                        streams should be set to 1. See samples README for\n",
      "                        more details.\n",
      "  --latency_percentile {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100}\n",
      "                        Optional. Defines the percentile to be reported in\n",
      "                        latency metric. The valid range is [1, 100]. The\n",
      "                        default value is 50 (median).\n",
      "  -enforcebf16 [{True,False}], --enforce_bfloat16 [{True,False}]\n",
      "                        Optional. By default floating point operations\n",
      "                        execution in bfloat16 precision are enforced if\n",
      "                        supported by platform. 'True' - enable bfloat16\n",
      "                        regardless of platform support. 'False' - disable\n",
      "                        bfloat16 regardless of platform support.\n",
      "  -nthreads NUMBER_THREADS, --number_threads NUMBER_THREADS\n",
      "                        Number of threads to use for inference on the CPU, GNA\n",
      "                        (including HETERO and MULTI cases).\n",
      "  -pin {YES,NO,NUMA,HYBRID_AWARE}, --infer_threads_pinning {YES,NO,NUMA,HYBRID_AWARE}\n",
      "                        Optional. Enable threads->cores ('YES' which is\n",
      "                        OpenVINO runtime's default for conventional CPUs),\n",
      "                        threads->(NUMA)nodes ('NUMA'), threads->appropriate\n",
      "                        core types ('HYBRID_AWARE', which is OpenVINO\n",
      "                        runtime's default for Hybrid CPUs) or completely\n",
      "                        disable ('NO') CPU threads pinning for CPU-involved\n",
      "                        inference.\n",
      "  -exec_graph_path EXEC_GRAPH_PATH, --exec_graph_path EXEC_GRAPH_PATH\n",
      "                        Optional. Path to a file where to store executable\n",
      "                        graph information serialized.\n",
      "  -pc [PERF_COUNTS], --perf_counts [PERF_COUNTS]\n",
      "                        Optional. Report performance counters.\n",
      "  -pcseq [PCSEQ], --pcseq [PCSEQ]\n",
      "                        Optional. Report latencies for each shape in\n",
      "                        -data_shape sequence.\n",
      "  -inference_only [INFERENCE_ONLY], --inference_only [INFERENCE_ONLY]\n",
      "                        Optional. If true inputs filling only once before\n",
      "                        measurements (default for static models), else inputs\n",
      "                        filling is included into loop measurement (default for\n",
      "                        dynamic models)\n",
      "  -report_type {no_counters,average_counters,detailed_counters}, --report_type {no_counters,average_counters,detailed_counters}\n",
      "                        Optional. Enable collecting statistics report.\n",
      "                        \"no_counters\" report contains configuration options\n",
      "                        specified, resulting FPS and latency.\n",
      "                        \"average_counters\" report extends \"no_counters\" report\n",
      "                        and additionally includes average PM counters values\n",
      "                        for each layer from the network. \"detailed_counters\"\n",
      "                        report extends \"average_counters\" report and\n",
      "                        additionally includes per-layer PM counters and\n",
      "                        latency for each executed infer request.\n",
      "  -report_folder REPORT_FOLDER, --report_folder REPORT_FOLDER\n",
      "                        Optional. Path to a folder where statistics report is\n",
      "                        stored.\n",
      "  -dump_config DUMP_CONFIG\n",
      "                        Optional. Path to JSON file to dump OpenVINO\n",
      "                        parameters, which were set by application.\n",
      "  -load_config LOAD_CONFIG\n",
      "                        Optional. Path to JSON file to load custom OpenVINO\n",
      "                        parameters. Please note, command line parameters have\n",
      "                        higher priority then parameters from configuration\n",
      "                        file.\n",
      "  -qb {8,16}, --quantization_bits {8,16}\n",
      "                        Optional. Weight bits for quantization: 8 (I8) or 16\n",
      "                        (I16)\n",
      "  -ip {u8,U8,f16,FP16,f32,FP32}, --input_precision {u8,U8,f16,FP16,f32,FP32}\n",
      "                        Optional. Specifies precision for all input layers of\n",
      "                        the network.\n",
      "  -op {u8,U8,f16,FP16,f32,FP32}, --output_precision {u8,U8,f16,FP16,f32,FP32}\n",
      "                        Optional. Specifies precision for all output layers of\n",
      "                        the network.\n",
      "  -iop INPUT_OUTPUT_PRECISION, --input_output_precision INPUT_OUTPUT_PRECISION\n",
      "                        Optional. Specifies precision for input and output\n",
      "                        layers by name. Example: -iop \"input:f16, output:f16\".\n",
      "                        Notice that quotes are required. Overwrites precision\n",
      "                        from ip and op options for specified layers.\n",
      "  -cdir CACHE_DIR, --cache_dir CACHE_DIR\n",
      "                        Optional. Enable model caching to specified directory\n",
      "  -lfile [LOAD_FROM_FILE], --load_from_file [LOAD_FROM_FILE]\n",
      "                        Optional. Loads model from file directly without\n",
      "                        read_network.\n",
      "  -iscale INPUT_SCALE, --input_scale INPUT_SCALE\n",
      "                        Optional. Scale values to be used for the input image\n",
      "                        per channel. Values to be provided in the [R, G, B]\n",
      "                        format. Can be defined for desired input of the model.\n",
      "                        Example: -iscale data[255,255,255],info[255,255,255]\n",
      "  -imean INPUT_MEAN, --input_mean INPUT_MEAN\n",
      "                        Optional. Mean values to be used for the input image\n",
      "                        per channel. Values to be provided in the [R, G, B]\n",
      "                        format. Can be defined for desired input of the model.\n",
      "                        Example: -imean data[255,255,255],info[255,255,255]\n",
      "\n",
      "Available target devices:   CPU  GPU\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a663c9",
   "metadata": {},
   "source": [
    "## 任務一: 使用benchmark app, 評估 Intel's Pre-Trained Model [human-pose-estimation-0001](https://docs.openvino.ai/latest/omz_models_model_human_pose_estimation_0001.html) 在CPU上的效能\n",
    "![\"Human Pose Estimation\"](https://docs.openvino.ai/latest/_images/human-pose-estimation-0001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15854ce",
   "metadata": {},
   "source": [
    "### 下載模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b06724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa02c4c5",
   "metadata": {},
   "source": [
    "### 執行benchmark 測試 for 模型 human-pose-estimation-0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad60ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 參考輸出結果 ###########\n",
    "# Count:          996 iterations\n",
    "# Duration:       60351.21 ms\n",
    "# Latency:\n",
    "#    Median:     245.30 ms\n",
    "#    AVG:        242.09 ms\n",
    "#    MIN:        185.44 ms\n",
    "#    MAX:        332.81 ms\n",
    "# Throughput: 16.50 FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368daa2",
   "metadata": {},
   "source": [
    "## 任務二: 使用benchmark app, 評估 Public Pre-Trained Model [yolo-v3-tf](https://docs.openvino.ai/latest/omz_models_model_yolo_v3_tf.html) 在CPU上的效能\n",
    "\n",
    "![\"smart_city_yolo-v3\"](https://i.imgur.com/W4VfRSB.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50b1fb",
   "metadata": {},
   "source": [
    "### 下載模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13014387",
   "metadata": {},
   "source": [
    "### 轉換模型成IR檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c740c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_converter --name \"yolo-v3-tf\" --download_dir \"model\" --precisions \"FP16\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecd423",
   "metadata": {},
   "source": [
    "### 執行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03c192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 參考輸出結果 ###########\n",
    "# Count:          1336 iterations\n",
    "# Duration:       121006.81 ms\n",
    "# Latency:\n",
    "#   Median:     671.10 ms\n",
    "#   AVG:        722.62 ms\n",
    "#   MIN:        117.08 ms\n",
    "#   MAX:        5508.58 ms\n",
    "# Throughput: 11.04 FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54020810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
